{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8741016-ef8c-44d6-a882-d58cd31cbdf3",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "This experiment uses custom modules in the *backend* folder, which have been specifically developed for this third-year lab. This notebook works directly with the **13 TeV 2025 ATLAS Open Data**. \n",
    "\n",
    "Please run the cell below to install the required packages. You will need to do this **each time you start the server**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad057554-013a-4380-b7d7-8477fa8ee0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install atlasopenmagic\n",
    "!pip install pyarrow==20.0.0\n",
    "from atlasopenmagic import install_from_environment\n",
    "install_from_environment(environment_file=\"../backend/environment.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13484bbb-be67-4443-af46-7cbacb9df9ec",
   "metadata": {},
   "source": [
    "Next, run the cell below to import the required modules and functions for the experiment. Repeat this step **every time you restart the kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4c54d-6689-434e-909c-06b02edf1be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import awkward as ak \n",
    "import vector\n",
    "import time\n",
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import hist\n",
    "from hist import Hist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from backend import analysis_uproot, DIDS_DICT, VALID_SKIMS, plot_stacked_hist, plot_histograms, analysis_parquet, plot_errorbars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c0b5a-dded-4703-b872-17259a201992",
   "metadata": {},
   "source": [
    "## Accessing Data Samples\n",
    "Data samples stored in the *backend* folder can be accessed using `skim` **and** a *string code*, passed to the `analysis_uproot` function. The key `skim` and string codes are listed below:  \n",
    "\n",
    "#### `Skim` (Final-State Filters)\n",
    "* `'2to4lep'` - Events with two to four leptons, each with at least 7 GeV of transverse momentum $p_T$\n",
    "* `'exactly4lep''` - Events with exactly four leptons, each with at least 7 GeV of transverse momentum $p_T$\n",
    "* `'GamGam'` - Events with at least two photons, each with at least 25 GeV of $p_T$\n",
    "\n",
    "#### String Codes\n",
    "* `'Data'` - Real data\n",
    "* `'Zee'` - Simulated $Z \\rightarrow e^+e^-$ events\n",
    "* `'Zmumu'` - Simulated $Z \\rightarrow \\mu^+\\mu^-$ events\n",
    "* `'Hyy'` - Simulated $H \\rightarrow \\gamma \\gamma$ events\n",
    "\n",
    "> **Note**: The string code for real data is always `'Data'`, regardless of the `skim` used.\n",
    "\n",
    "To combine multiple datasets, combine the string codes using `'+'`. For example, if you would like to combine the $Z \\rightarrow \\mu^+\\mu^-$ and $Z \\rightarrow e^+e^-$ datasets, use the string code `'Zee+Zmumu'`.\n",
    "\n",
    "All available `skim` can be viewed with `VALID_SKIMS`, and all string codes with `DIDS_DICT.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f4ee7-0bb2-44fb-9848-bef167568420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VALID_SKIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b76f6-9b75-425f-b84d-d8baecf3c8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIDS_DICT.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f35561-3ee1-444f-b369-8d07eeee24d4",
   "metadata": {},
   "source": [
    "To use `analysis_uproot`, the following arguments must be provided:\n",
    "- `string_code_dict` (*dict*) : Maps dataset labels to string codes\n",
    "  - **Keys**: Labels used in the output dictionary of Awkward Arrays  \n",
    "        - Must include 'Data' in the key for real data. When detected, `analysis_uproot` will skip weight calculation for that sample to reduce memory usage.\n",
    "  - **Values**: The corresponding string codes\n",
    "  - **Example**:\n",
    "    ```\n",
    "    string_code_dict = {\n",
    "        'Data' : 'Data',\n",
    "        'Signal Zee' : 'Zee',\n",
    "        'Background Wenu+Wmunu' : 'Wenu+Wmunu'\n",
    "    }\n",
    "    ```\n",
    "- `skim` (*str*) e.g. `'2to4lep'`\n",
    "- `luminosity` (*float*) : Integrated luminosity in fb⁻¹, e.g. `36.6`.\n",
    "- `fraction` (*float*) : Fraction of each dataset to read, e.g. `0.1` for 10%\n",
    "- `read_variables` (*list of str*) : Names of ROOT branches to read\n",
    "- `save_variables`(*list of str*) : Variables to keep in memory or write to disk\n",
    "\n",
    "Optional argument:\n",
    "- `cut_function` (*callable*) : Function that receives an argument and returns it. Can be used to:\n",
    "    - Apply selection cuts\n",
    "    - Compute derived variables using the data in `read_variables`, e.g. invariant mass\n",
    "- `local_files` (*bool*) : Set to `False` to stream the sample files, `True` to access the local files\n",
    "- `sample_path` (*str*) : Path to the sample files, either for accessing existing files or downloading new ones (default: `'../backend/datasets'`)\n",
    "- `write_parquet` (*bool*) : Set to `True` to write output to Parquet files\n",
    "- `output_directory` (*str or None*) :\n",
    "   - Directory where files will be saved\n",
    "   - If `None`, a unique folder in `output/` will be created using the current date and time\n",
    "- `write_txt` (*bool*) : Set to `True` to write a summary log of the run\n",
    "- `txt_filename` (*str or None*) : Filename for the summary log. If not provided, a unique file will be created in the `txt/` folder.\n",
    "- `return_output` (*bool*)\n",
    "  - `True` (default) : Returns the output dictionary of Awkward Arrays\n",
    "  - `False` : Nothing is returned (saves memory when only writing to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9d8f0-0317-45f5-ba8e-bfef1bdced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate weights\n",
    "string_code_dict = {\n",
    "#    'Data' : 'Data',\n",
    "#    'Signal Zee' : 'Zee',\n",
    "    'Signal Zmumu' : 'Zmumu',\n",
    "#    'Background Wenu+Wmunu' : 'Wenu+Wmunu',\n",
    "#    'Background VV4l+H4l' : 'VV4l+H4l'\n",
    "}\n",
    "skim = '2to4lep'\n",
    "luminosity = 36.6\n",
    "fraction = 0.01\n",
    "read_variables = ['lep_n', 'lep_pt', 'lep_type',  'lep_isLooseID',\n",
    " 'lep_isMediumID',\n",
    " 'lep_isTightID',\n",
    " 'lep_isLooseIso',\n",
    " 'lep_isTightIso',\n",
    " 'filteff', 'kfac', 'xsec', 'mcWeight', 'ScaleFactor_PILEUP',  'ScaleFactor_ELE', 'ScaleFactor_MUON', 'ScaleFactor_LepTRIGGER']\n",
    "\n",
    "save_variables = read_variables\n",
    "\n",
    "def pt_30_cut(data):\n",
    "    data = data[data['lep_n'] == 2]\n",
    "    \n",
    "    pt_cut = (data['lep_pt'][:, 0] > 15) & (data['lep_pt'][:, 1] > 30)\n",
    "    data = data[pt_cut]\n",
    "    type_cut = (data['lep_type'][:, 0] + data['lep_type'][:, 1] ==26)\n",
    "    data = data[type_cut]\n",
    "\n",
    "    return data\n",
    "\n",
    "pt30_data = analysis_uproot(skim, string_code_dict, luminosity, fraction, \n",
    "                            read_variables, save_variables, cut_function=pt_30_cut)\n",
    "pt30_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0129c36-e636-4d31-8079-28a4dfad6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate ordering of pt vector\n",
    "string_code_dict = {\n",
    "#    'Data' : 'Data',\n",
    "#    'Signal Zee' : 'Zee',\n",
    "#    'Signal Zmumu' : 'Zmumu',\n",
    "    'Signal H4l' : 'H4l',\n",
    "#    'Background Wenu+Wmunu' : 'Wenu+Wmunu',\n",
    "#    'Background VV4l+H4l' : 'VV4l+H4l'\n",
    "    'Background VV4l' : 'VV4l'\n",
    "}\n",
    "#skim = '2to4lep'\n",
    "skim = 'exactly4lep' \n",
    "luminosity = 36.6\n",
    "fraction = 0.3\n",
    "read_variables = ['lep_n', 'lep_pt', 'lep_type']\n",
    "\n",
    "save_variables = read_variables\n",
    "\n",
    "def pt_30_cut(data):\n",
    "    data = data[data['lep_n'] == 4]\n",
    " \n",
    "    data['sum_lep_type'] = data['lep_type'][:, 0] + data['lep_type'][:, 1] + data['lep_type'][:, 2] + data['lep_type'][:, 3] \n",
    "    # Select events with correctly matched pairs of opposite-sign same-flavour leptons \n",
    "    data = data[(data['sum_lep_type']!=44) & (data['sum_lep_type']!=52)]\n",
    "\n",
    "    data['pt_diff_1'] = data['lep_pt'][:, 0] - data['lep_pt'][:, 1]\n",
    "    data['pt_diff_2'] = data['lep_pt'][:, 1] - data['lep_pt'][:, 2]\n",
    "    data['pt_diff_3'] = data['lep_pt'][:, 2] - data['lep_pt'][:, 3]\n",
    "\n",
    "    return data\n",
    "    \n",
    "pt30_data = analysis_uproot(skim, string_code_dict, luminosity, fraction, \n",
    "                            read_variables, save_variables, cut_function=pt_30_cut)\n",
    "pt30_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f4ccd-7964-4b2e-8a79-20fb7db302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preselection to make the default '2to4lep' parquet files\n",
    "string_code_dict = {\n",
    "#    '2to4lep' : 'Data',\n",
    "#    'H4l' : 'H4l',\n",
    "#    'VV4l' : 'VV4l',\n",
    "#    'Zee' : 'Zee',\n",
    "#    'Zmumu' : 'Zmumu',\n",
    "    'm10_40_Zee' : 'm10_40_Zee', \n",
    "    'm10_40_Zmumu' : 'm10_40_Zmumu', \n",
    "    'm10_40_Ztautau' : 'm10_40_Ztautau',\n",
    "#    'ttbar' : 'ttbar',\n",
    "#    'Wenu' : 'Wenu',\n",
    "#    'Wmunu' : 'Wmunu',\n",
    "#    'Ztautau' : 'Ztautau',\n",
    "#    'Wtaunu' : 'Wtaunu',\n",
    "#    'ttV' : 'ttV',\n",
    "#    'VVV' : 'VVV',\n",
    "    'VV3l' : 'VV3l',\n",
    "    'VBF_Zll' : 'VBF_Zll',\n",
    "}\n",
    "\n",
    "skim = '2to4lep'\n",
    "luminosity = 36.6\n",
    "fraction = 1\n",
    "\n",
    "read_variables = ['lep_n',\n",
    " 'lep_pt',\n",
    " 'lep_eta',\n",
    " 'lep_phi',\n",
    " 'lep_e',\n",
    " 'lep_ptvarcone30',\n",
    " 'lep_topoetcone20',\n",
    " 'lep_type',\n",
    " 'lep_charge',\n",
    " 'lep_isLooseID',\n",
    " 'lep_isMediumID',\n",
    " 'lep_isTightID',\n",
    " 'lep_isLooseIso',\n",
    " 'lep_isTightIso',\n",
    " 'trigE',\n",
    " 'trigM',\n",
    " 'lep_isTrigMatched']\n",
    "\n",
    "save_variables = read_variables\n",
    "\n",
    "def preselection_2to4lep_cut(data):\n",
    "    # Preselection cuts for writing out skim = '2to4lep' parquet files\n",
    "    # Require single-lepton trigger and at least one trigger-matched lepton with pt > 27 GeV\n",
    "\n",
    "    \n",
    "    # Require that a single lepton trigger fired \n",
    "    data = data[(data['trigM'] | data['trigE'])] \n",
    "\n",
    "    # Require at least one lepton to have  pt > 27 GeV and be trigger matched (\n",
    "    data = data[((data['lep_pt'][:, 0] > 27) & (data['lep_isTrigMatched'][:, 0])) | ((data['lep_pt'][:, 1] > 27) & (data['lep_isTrigMatched'][:, 1])) | (data['lep_n'] > 2)]\n",
    "    return data\n",
    "\n",
    "analysis_uproot(skim, string_code_dict, luminosity, fraction,\n",
    "                            read_variables, save_variables, local_files=True, cut_function=preselection_2to4lep_cut,output_directory='../backend/parquet2', write_parquet=True, return_output=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14f69b-197d-4c5b-8fa8-1e69301af591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_code_dict = {\n",
    "#    'Data' : 'Data',\n",
    "#    'Signal Zee' : 'Zee',\n",
    "    'Signal Zmumu' : 'Zmumu',\n",
    "#    'Background Wenu+Wmunu' : 'Wenu+Wmunu',\n",
    "#    'Background VV4l+H4l' : 'VV4l+H4l'\n",
    "}\n",
    "skim = '2to4lep'\n",
    "luminosity = 36.6\n",
    "fraction = 0.005\n",
    "read_variables = ['lep_n', 'lep_pt',\n",
    "                 'lep_eta', 'lep_phi', 'lep_e']\n",
    "save_variables = read_variables\n",
    "\n",
    "def pt_30_cut(data):\n",
    "    data = data[data['lep_n'] == 2]\n",
    "    \n",
    "    pt_cut = (data['lep_pt'][:, 0] > 30) & (data['lep_pt'][:, 1] > 30)\n",
    "    data = data[pt_cut]\n",
    "    # Define four momentum\n",
    "    four_momentum = vector.zip({\n",
    "        'pt': data['lep_pt'],\n",
    "        'eta' : data['lep_eta'],\n",
    "        'phi' : data['lep_phi'],\n",
    "        'E' : data['lep_e']\n",
    "    })\n",
    "    # Add the 4-momentum of the two leptons in each event and get the \n",
    "    # invariant mass using .M\n",
    "    data['mass'] = (four_momentum[:, 0] + four_momentum[:, 1]).M\n",
    "    return data\n",
    "\n",
    "pt30_data = analysis_uproot(skim, string_code_dict, luminosity, fraction, \n",
    "                            read_variables, save_variables, cut_function=pt_30_cut)\n",
    "pt30_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ea872-5e04-4eb6-8f0a-db9377468482",
   "metadata": {},
   "source": [
    "You can simply set `write_parquet = True` if you would like to write data to disk. It is recommended to set `return_output = False` to reduce memory usage if you choose to save output to Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6acb0e-73c4-4282-9970-6eda3c03a2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pt_20_cut(data):\n",
    "    data = data[data['lep_n'] == 2]\n",
    "\n",
    "    pt_cut = (data['lep_pt'][:, 0] > 20) & (data['lep_pt'][:, 1] > 20)\n",
    "    data = data[pt_cut]\n",
    "    \n",
    "    # Define four momentum\n",
    "    four_momentum = vector.zip({\n",
    "        'pt': data['lep_pt'],\n",
    "        'eta' : data['lep_eta'],\n",
    "        'phi' : data['lep_phi'],\n",
    "        'E' : data['lep_e']\n",
    "    })\n",
    "    # Add the 4-momentum of the two leptons in each event and get the \n",
    "    # invariant mass using .M\n",
    "    data['mass'] = (four_momentum[:, 0] + four_momentum[:, 1]).M\n",
    "    return data\n",
    "\n",
    "output_dir = 'output/pt20cut'\n",
    "pt20_data = analysis_uproot(skim, string_code_dict, luminosity, fraction, \n",
    "                            read_variables, save_variables, cut_function=pt_20_cut,\n",
    "                            write_txt=False, txt_filename=None, local_files=True,\n",
    "                            write_parquet=True, output_directory=output_dir,\n",
    "                            return_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03f08d-3872-47cb-8f9f-fffd002cf112",
   "metadata": {},
   "source": [
    "## Reading Parquet Files\n",
    "You can read the files saved by `analysis_uproot` using the `analysis_parquet` function. Specify the directory you want to read from using the `read_directory` keyword argument. If `subdirectory_names` is not provided, the function will attempt to read all subfolders.  \n",
    "_**Tips**_: For large datesets, it is recommended to restart the kernel beforehand to clear memory. Do not read too many variables at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbec465-9276-46c8-9825-170aa587c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_directory = 'output/pt20cut'\n",
    "read_variables = ['lep_pt', 'mass']\n",
    "pt20_data = analysis_parquet(read_variables, \n",
    "                             read_directory=read_directory, \n",
    "                             subdirectory_names=None,\n",
    "                             fraction=1, cut_function=None, \n",
    "                             write_parquet=False,\n",
    "                             output_directory=None, return_output=True)\n",
    "pt20_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6211898-3d5a-4d8d-80d8-ad82ec6c45d4",
   "metadata": {},
   "source": [
    "The same function can be used to apply selection cuts and write the filtered data to disk. Note that the dictionary are constructed by combining the subdirectory names with the `fraction`, separated by `' x'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03501c-c879-4dc1-8ae7-6ba9a7b1135f",
   "metadata": {},
   "source": [
    "## Plotting Stacked Histograms\n",
    "The dictionary returned by `analysis_uproot` can be passed directly to `plot_stacked_hist` to plot stacked histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe4613-fd4c-4dc1-bd0b-629dd20d6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to plot on the x-axis\n",
    "plot_variable = 'pt_diff_3'\n",
    "\n",
    "# Define plot appearance\n",
    "color_list = ['b', 'r']\n",
    "xmin, xmax = -100, 100 # Define histogram bin range and x-axis limits \n",
    "num_bins = 200 # Number of histogram bins\n",
    "x_label = 'pt[2] - pt[3] [GeV]' # x-axis label \n",
    "\n",
    "# Plot the histogram\n",
    "fig, hists = plot_stacked_hist(pt30_data, plot_variable, color_list,\n",
    "                               num_bins, xmin, xmax, x_label,\n",
    "                               show_text=False, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc47027-6c7d-4a33-8d51-248a90b8bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to plot on the x-axis\n",
    "plot_variable = 'mass'\n",
    "\n",
    "# Define plot appearance\n",
    "color_list = ['k', 'yellow', 'purple', 'g', 'm']\n",
    "xmin, xmax = 0, 120 # Define histogram bin range and x-axis limits \n",
    "num_bins = 500 # Number of histogram bins\n",
    "x_label = 'mass [GeV]' # x-axis label \n",
    "\n",
    "# Plot the histogram\n",
    "fig, hists = plot_stacked_hist(pt30_data, plot_variable, color_list,\n",
    "                               num_bins, xmin, xmax, x_label,\n",
    "                               show_text=False, save_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd69d23-1817-42b1-b818-f5de7226aaa7",
   "metadata": {},
   "source": [
    "If you would like to plot many variables at once, you can use the `plot_histograms` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb2128-3be9-4afc-86a4-b0687db42744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot many variables at once \n",
    "plot_variables = ['lep_type[0]','lep_type[1]','lep_type[2]','lep_type[3]']\n",
    "xmin_xmax_list = (10.5, 13.5) # Bin range for all variables\n",
    "# Define plot appearance\n",
    "color_list = ['b', 'r']\n",
    "num_bins_list = 3\n",
    "x_label_list = plot_variables\n",
    "\n",
    "figure_list, hists_list = plot_histograms(pt30_data,\n",
    "                                          plot_variables,\n",
    "                                          color_list,\n",
    "                                          xmin_xmax_list,\n",
    "                                          num_bins_list,\n",
    "                                          x_label_list, logy=False, residual_plot=False, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903244ff-747a-4742-9437-3e635ec8a65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot many variables at once \n",
    "plot_variables = [ 'lep_isLooseID[0]',\n",
    " 'lep_isMediumID[0]',\n",
    " 'lep_isTightID[0]',\n",
    " 'lep_isLooseIso[0]',\n",
    " 'lep_isTightIso[0]',\n",
    "'totalWeight', 'mcWeight', 'filteff', 'kfac', 'xsec', 'ScaleFactor_PILEUP',  'ScaleFactor_ELE', 'ScaleFactor_MUON', 'ScaleFactor_LepTRIGGER']\n",
    "xmin_xmax_list = [(-0.05, 1.5), (-0.05, 1.5), (-0.05, 1.5), (-0.05, 1.5), (-0.05, 1.5), (-20, 20), (-500000000, 500000000), (0,1), (1,2), (2000,2500), (0,5), (0,5), (0,5), (0,5)] # Bin range for all variables\n",
    "# Define plot appearance\n",
    "color_list = ['b']\n",
    "num_bins_list = 200\n",
    "x_label_list = ['lep_isLooseID[0]',\n",
    " 'lep_isMediumID[0]',\n",
    " 'lep_isTightID[0]',\n",
    " 'lep_isLooseIso[0]',\n",
    " 'lep_isTightIso[0]','totalWeight', 'mcWeight', 'filteff', 'kfac', 'xsec', 'ScaleFactor_PILEUP',  'ScaleFactor_ELE', 'ScaleFactor_MUON', 'ScaleFactor_LepTRIGGER']\n",
    "\n",
    "figure_list, hists_list = plot_histograms(pt30_data,\n",
    "                                          plot_variables,\n",
    "                                          color_list,\n",
    "                                          xmin_xmax_list,\n",
    "                                          num_bins_list,\n",
    "                                          x_label_list, logy=False, residual_plot=False, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef67f5-45ba-4057-bf0a-c3580596bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the Figure object returned by the function to save the figure as an image\n",
    "figure_list[0].savefig('test.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d12d20-d2e9-4b4f-8c1b-dccdc5ccf628",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_list[1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
