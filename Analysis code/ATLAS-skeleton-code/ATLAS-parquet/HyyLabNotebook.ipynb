{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043f9ec3-b8e2-4e53-9c9a-f907af7b185f",
   "metadata": {},
   "source": [
    "# About Jupyter Notebook\n",
    "For some of you, this might be your first time using a Jupyter notebook. Here are a few tips to get you started!\n",
    "\n",
    "A Jupyter notebook is a file with the .ipynb extension, just like this one. It's made up of *cells*, which can be:\n",
    "- Code cells - where you write your code\n",
    "- Markdown cells - where you write formatted text (like this section!)\n",
    "\n",
    "Code cell are labeled with `[ ]` on the left. When a cell is running, the label changes to `[*]`. The number inside the brackets shows the order in which cells have been executed by the *kernel*.\n",
    "\n",
    "So, what is the kernel? The kernel is the computational 'engine' that runs the code. This notebook is connected to a Python kernel.\n",
    "\n",
    "### Useful Keyboard Shortcuts\n",
    "Jupyter notebooks offer helpful keyboard shortcuts that may save your time. First, make sure you are in *command mode* (not editing a cell). Press <kbd>Esc</kbd> or click outside the cell to exit *edit mode*.\n",
    "\n",
    "Here are some handy shortcuts:\n",
    "- Insert a new cell above or below → press <kbd>A</kbd> or <kbd>B</kbd>\n",
    "- Delete a cell → press <kbd>D</kbd> twice (Note: Deleted cells cannot be recovered!)\n",
    "- Restart the kernel and run all cells → press <kbd>0</kbd> twice (this is a zero!)\n",
    "- Run the selected cell → press <kbd>Ctrl</kbd> + <kbd>Enter</kbd>\n",
    "- Run the selected cell and move to the next cell → press <kbd>Shift</kbd> + <kbd>Enter</kbd>\n",
    "\n",
    "Please check out the toolbar and menu bar at the top of the notebook for more options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcca48-33fc-4422-91bf-f61283222400",
   "metadata": {},
   "source": [
    "# Awkward Arrays\n",
    "This lab uses a library called **Awkward Array**. Like Numpy, Awkward Arrays can have multiple dimensions. However, unlike Numpy, Awkward arrays support dimensions with *varying lengths*, making them ideal for handling irregular or nested data in particle physics experiments.\n",
    "### Structured Data in Awkward Arrays\n",
    "This lab uses structured data, where each element is a *record* with named fields (similar to a dictionary). For example:\n",
    "```\n",
    "array1 = ak.Array([{\n",
    "    'a': [1, 2, 3],\n",
    "    'b': 4,\n",
    "    'c': [5, 6]\n",
    "}])\n",
    "```\n",
    "This array `array1` contains *one* record with three fields: `'a'`, `'b'`, and `'c'`. You can access the list of fields with `array1.fields`.\n",
    "### Concatenating Awkward Arrays\n",
    "To combine two Awkward arrays with matching structure, use `ak.concatenate()`. An example:\n",
    "```\n",
    "array2 = ak.Array([{\n",
    "    'a': [7, 8],\n",
    "    'b': 9,\n",
    "    'c': [10, 11, 12]\n",
    "}])\n",
    "\n",
    "ak.concatenate([array1, array2])\n",
    "```\n",
    "This results in a single Awkward Array with *two* records:\n",
    "```\n",
    "[\n",
    "    {'a': [1, 2, 3], 'b': 4, 'c': [5, 6]},\n",
    "    {'a': [7, 8],    'b': 9, 'c': [10, 11, 12]}\n",
    "]\n",
    "```\n",
    "For more information about Awkward Arrays: https://awkward-array.org/doc/main/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a081d97-05f2-4e03-bea0-aeb2b493ae40",
   "metadata": {},
   "source": [
    "# The Experiment\n",
    "This experiment uses custom modules in the *backend* folder, which have been specifically developed for this third-year lab. This notebook works with **pre-processed data** derived from the **13 TeV 2025 ATLAS Open Data**.\n",
    "\n",
    "Please run the cell below to install the required packages. You will need to do this **each time you start the server**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134dc01-a41c-41b7-b327-2d167cf7ab86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install atlasopenmagic\n",
    "!pip install pyarrow==20.0.0\n",
    "\n",
    "from atlasopenmagic import install_from_environment\n",
    "install_from_environment(environment_file=\"../backend/environment.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a381be-130e-4341-b8ee-a1e191e49d67",
   "metadata": {},
   "source": [
    "The above cell is likely to take about 30 seconds to run. Wait until the [*] to the left of the above cell becomes a number, indicating that it has finished running. Then, run the cell below to import the required modules and functions for the experiment. Repeat this step **every time you restart the kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4c54d-6689-434e-909c-06b02edf1be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import awkward as ak\n",
    "import time\n",
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import uproot\n",
    "import glob\n",
    "import numpy as np\n",
    "import vector\n",
    "import hist\n",
    "from hist import Hist\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "from lmfit import fit_report\n",
    "from lmfit.models import PolynomialModel, GaussianModel\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from backend import get_valid_variables, validate_read_variables\n",
    "from backend import plot_stacked_hist, plot_histograms, histogram_2d, plot_errorbars\n",
    "from backend import get_histogram, analysis_parquet, VALID_STR_CODE, produced_event_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d3f6f-6971-40a1-a03f-fbc25821b8af",
   "metadata": {},
   "source": [
    "## Accessing Data Samples\n",
    "Data samples stored in the *backend* folder can be accessed using a *string code*. The key string codes are listed below:  \n",
    "#### Available Final-State Collections (Real Data)\n",
    "* `'2to4lep'` - Events with two to four leptons, each with at least 7 GeV of transverse momentum $p_T$\n",
    "* `'GamGam'` - Events with at least two photons, each with at least 25 GeV of $p_T$\n",
    "\n",
    "#### Available Monte Carlo Simulation Datasets\n",
    "* `'Zee'` - Simulated $Z \\rightarrow e^+e^-$ events\n",
    "* `'Zmumu'` - Simulated $Z \\rightarrow \\mu^+\\mu^-$ events\n",
    "* `'Hyy'` - Simulated $H \\rightarrow \\gamma \\gamma$ events\n",
    "\n",
    "You can use `VALID_STR_CODE` to view all available string codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36e8bb-47e5-4366-8c0f-c8334b69b254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VALID_STR_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b2fbb-90ab-48f5-82e8-f55bd895e81c",
   "metadata": {},
   "source": [
    "To combine multiple datasets, combine the string codes using `'+'`. For example, if you would like to combine the $Z \\rightarrow \\mu^+\\mu^-$ and $Z \\rightarrow e^+e^-$ datasets, use the string code `'Zee+Zmumu'`.\n",
    "\n",
    "The sample files in the backend contain a few important variables. You can view the available variables using the `get_valid_variables` function that is defined in the backend. An example is shown in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e31b7f-adf5-421d-bc40-8ec78a836cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_code = 'GamGam'\n",
    "get_valid_variables(string_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627a445-b99d-4793-9b9f-6ca748df1236",
   "metadata": {},
   "source": [
    "The `analysis_parquet` function reads datasets specified by the `string_code_list` input and returns a dictionary containing the selected data. The parameter `fraction` determines the fraction of each dataset to load; the default value is `1`. For Higgs analysis, it is recommended to start with `fraction = 0.1` or lower to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb4d91-218d-4ed0-bdd0-cb787841a2e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_code_list = ['GamGam', 'Hyy'] # List of dataset codes to load\n",
    "read_variables = ['photon_n', 'photon_pt']\n",
    "fraction = 0.1 # Fraction of each dataset to load\n",
    "# Call analysis_parquet to read the datasets and return the data as a dictionary\n",
    "data = analysis_parquet(read_variables, string_code_list, fraction=fraction)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f9665-6168-473c-b5c0-9e56bbadcbce",
   "metadata": {},
   "source": [
    "As shown in the example above, the `analysis_parquet` function returns a dictionary where:\n",
    "- **Keys** are constructed by combining each dataset code from `string_code_list` with the `fraction` (e.g. for `GamGam` and `fraction = 0.1`, the key is `'GamGam_0_1`)\n",
    "- **Values** are Awkward Arrays, where each entry corresponds to an event represented as a record (similar to a Python dictionary). The fields in each record correspond to the variables listed in `read_variables`.\n",
    "\n",
    "For example, the dictionary returned by the `analysis_parquet` may look like this:\n",
    "```\n",
    "data = {\n",
    "    'GamGam_0_1': Array([\n",
    "        {'photon_n': 2, 'photon_pt': [106, 47.6]},\n",
    "        {'photon_n': 2, 'photon_pt': [41.6, 37.3]},\n",
    "    ]),\n",
    "    'Hyy_0_1': Array([\n",
    "        ...\n",
    "    ])\n",
    "}\n",
    "```\n",
    "\n",
    "Let's interpret the first event in the `'GamGam_0_1'` dataset, which is represented by the first record in the Awkward Array stored under the `'GamGam_0_1'` key (i.e., `data['GamGam_0_1']`).\n",
    "- The number of photons in this event is 2 (`photon_n : 2`)\n",
    "- The field `photon_pt` lists their transverse momenta: The first photon has $p_T = 106$ GeV, and the second photon has $p_T = 47.6$ GeV.\n",
    "\n",
    "To extract the $p_T$ of the first photon from all events in the `'GamGam_0_1'` dataset, use  \n",
    "`data['GamGam_0_1']['photon_pt'][:, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c34f22-a15d-42b1-b38b-67674d7fac7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['GamGam_0_1']['photon_pt'][:, 0] # pT of the first photon from all events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39216841-0faf-40ca-a879-d0dfaf21ab8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Event Weights\n",
    "For a variety of reasons each event in the MC data has an associated *weight*.  \n",
    "\n",
    "When an event from the ATLAS experiment data is added to one of your histograms then the relevant histogram bin is incremented by 1.0.  However, when using MC events the relevant histogram bin is incremented by an amount `'totalWeight'`.  \n",
    "\n",
    "You have access to 36 fb$^{-1}$ of real ATLAS data.  However, the MC samples for most of the physical processes have a larger number of events than would be expected in 36 fb$^{-1}$ of data.  The application of a weight (typically less than one) ensures that the total number of events you see in MC histograms is equal to that predicted for the equivalent ATLAS data set.  This calculation involves knowing the predicted cross section, $\\sigma$, for each considered physical process and the integrated luminosity, $\\int\\mathcal{L} dt$.  The *weight* also corrects for discrepancies in experimental efficiencies between the MC simulation and the real ATLAS detector.\n",
    "\n",
    "You don't need to explicitly include `'totalWeight'` in `read_variables`. If the field is present in the data (i.e. in MC samples), `analysis_parquet` will return it automatically.\n",
    "\n",
    "Take a look the `EventWeights` module in the backend folder if you're curious how the variable is calculated!\n",
    "\n",
    "**_Tip_**: Please be careful not to edit any of the code in the backend folder – unless you really know what you are doing you could cause the code to perform in unexpected ways!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa601cc-92aa-41d5-aeac-6156d22ad973",
   "metadata": {},
   "source": [
    "## Event Selection\n",
    "\n",
    "You can also perform event selection by defining a custom *cut function*, which can be passed to `analysis_parquet` via the `cut_function` argument. This function takes the full dataset as input and returns a filtered version according to your selection criteria. In addition, you can compute new variables and store them as new fields in the event records within the cut function. \n",
    "\n",
    "The example below selects events with exactly two final-state photons. It then computes the invariant mass by summing their four-momentum vectors and accessing the `.M` attribute as implemented in the `yy_cut` function. This function is passed to the `analysis_parquet` via the `cut_function` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac462563-1e06-4a18-b099-174debb8c2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "string_code_list = ['GamGam', 'Hyy'] # List of dataset codes to load\n",
    "\n",
    "# Variables to read from the dataset\n",
    "read_variables = ['photon_pt', 'photon_eta', 'photon_phi', 'photon_e', 'photon_n']\n",
    "\n",
    "# You may also use the pre-defined function `validate_read_variables` to validate your `read_variables`.\n",
    "read_variables = validate_read_variables(string_code_list, read_variables)\n",
    "\n",
    "# Custom selection cut function to filter the data\n",
    "def yy_cut(data):\n",
    "    # Cut on photon number\n",
    "    cut_photon_n = (data['photon_n'] == 2) # Define mask\n",
    "    data = data[cut_photon_n] # Only keep events that have two photons\n",
    "\n",
    "    # Cut on photon pt\n",
    "    photon_pt0 = data['photon_pt'][:, 0] \n",
    "    photon_pt1 = data['photon_pt'][:, 1]\n",
    "    # Use bitwise operator '&' for AND, '|' for OR. Remember the parentheses!\n",
    "    cut_photon_pt = (photon_pt0 > 20) & (photon_pt1 > 20)\n",
    "    data = data[cut_photon_pt] # Keep events where two photons have pt > 20 GeV\n",
    "\n",
    "    # Define four momentum\n",
    "    four_momentum = vector.zip({\n",
    "        'pt': data['photon_pt'],\n",
    "        'eta' : data['photon_eta'],\n",
    "        'phi' : data['photon_phi'],\n",
    "        'E' : data['photon_e']\n",
    "    })\n",
    "    # Add the 4-momentum of the two photons in each event and get the \n",
    "    # invariant mass using .M\n",
    "    data['mass'] = (four_momentum[:, 0] + four_momentum[:, 1]).M\n",
    "    \n",
    "    return data\n",
    "\n",
    "fraction = 0.5\n",
    "data = analysis_parquet(read_variables, string_code_list, fraction=fraction, cut_function=yy_cut)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e375f78-6005-438c-99ee-e49556ec3384",
   "metadata": {},
   "source": [
    "If you only need data for a specific photon — for example, the $p_T$ of the first photon — you can include `'photon_pt[0]'` in `read_variables` instead of `'photon_pt'`. This saves memory by storing the data as a flat array rather than a nested one. However, do this only if `'photon_pt'` is not required elsewhere, such as in the cut function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4dfe6-323d-48e5-92b2-a5e1df00be1d",
   "metadata": {},
   "source": [
    "## Plot Stacked Histogram\n",
    "The dictionary returned by `analysis_parquet` can be used to plot a stacked histogram by passing it to the `plot_stacked_hist` function.\n",
    "\n",
    "To use this function correctly, there are a few points to take note of:\n",
    "- The first argument must be a dictionary.\n",
    "- If a dictionary key includes `'Data'`, its content will be plotted as data points with error bars.\n",
    "- If a dictionary key includes `'Signal'`, its content will be stacked as histogram bars on top of the background.\n",
    "- Python strings are case-sensitive, i.e. `'Data'` and `'data'` are treated differently!\n",
    "\n",
    "An example is shown below, where `'GamGam'` is treated as data and `'Hyy'` as the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdd532-8692-4806-a159-07efe5f0db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what to plot: 'GamGam' as data, 'Hyy' as signal\n",
    "plot_dict = {\n",
    "    'Data' : data['GamGam_0_5'],\n",
    "    'Signal' : data['Hyy_0_5']\n",
    "}\n",
    "\n",
    "# Variable to plot on the x-axis\n",
    "plot_variable = 'mass'\n",
    "\n",
    "# Define plot appearance\n",
    "color_list = ['k', 'b'] # Black (data), blue (signal)\n",
    "xmin, xmax = 0, 200 # Define histogram bin range and x-axis limits \n",
    "num_bins = 200 # Number of histogram bins\n",
    "x_label = 'mass [GeV]' # x-axis label \n",
    "\n",
    "# Plot the histogram\n",
    "fig, hists = plot_stacked_hist(plot_dict, plot_variable, color_list, num_bins, xmin, xmax, x_label, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0600c08-b14c-471c-91bb-e2c77f0f0107",
   "metadata": {},
   "source": [
    "The `plot_stacked_hist` function accepts several optional parameters that allow you to customise the appearance and behaviour of the plot:\n",
    "- `y_label` : Label for the y-axis\n",
    "- `ylim` : Tuple of two numbers for y-axis limits\n",
    "- `logy` : Set to `True` to use a logarithmic y-axis\n",
    "- `title` : Title of the plot\n",
    "- `marker` : Marker style for the data points (default: `'o'`)\n",
    "- `fig_size` : Tuple of two numbers for the figure size (default: `(12, 8)`)\n",
    "- `show_text` : Set to `True` to display text annotations (histogram information) on the plot\n",
    "- `show_back_unc` : Set to `False` to hide background uncertainty\n",
    "- `save_fig` : Set to `True` to save the figure\n",
    "- `fig_name` : String filename to save the plot as an image\n",
    "- `residual_plot` : Set to `True` to add a residual plot (Data / MC) below the main plot\n",
    "- `residual_plot_ylim` : Tuple of two numbers for residual plot y-axis limits\n",
    "- `title_fontsize` : Font size for the plot title (default: 17)\n",
    "- `label_fontsize` : Font size for the x and y axis labels (default: 17)\n",
    "- `legend_fontsize` : Font size for the legend (default: 17)\n",
    "- `tick_labelsize` : Font size for the axis tick labels  (default: 15)\n",
    "- `text_fontsize` : Font size for text annotations (default: 14)\n",
    "\n",
    "The function returns a `Figure` object and a **list** of `Hist` objects corresponding to each dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57201a6f-03f6-4627-8531-bbbbe00a7c08",
   "metadata": {},
   "source": [
    "## Hist\n",
    "`Hist` objects can be used to inspect the histogram contents, including the bin values, variances (for MC datasets), and underflow/overflow bins.  \n",
    "The second value returned by `plot_stacked_hist` is a list of `Hist` objects. Their order matches the order of the string codes in `string_code_list`.\n",
    "\n",
    "Here are some useful methods:\n",
    "- `.sum()` - total sum of all bin contents\n",
    "- `.view()` - bin values (Data)\n",
    "- `.view().value` - bin values (MC)\n",
    "- `.view().variance` - bin variance (MC)\n",
    "- `.view(flow=True)[0]` - underflow bin value\n",
    "- `.view(flow=True)[-1]` - overflow bin value\n",
    "- `.axes[0].centers` - bin centres\n",
    "  \n",
    "For example, try uncommenting the lines below to explore the contents of a `Hist` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d101e8d-1864-4a77-8f46-8db00fb0cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, the first Hist object (index 0) is Data, and the second (index 1) is MC\n",
    "hists                 # View the full histogram object\n",
    "#hists[0]                 # View the full histogram object\n",
    "# hists[0].sum()         # Total sum of all bin contents\n",
    "# hists[0].view()        # Access the bin values (excluding flow bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0f0b7-b977-4ffe-b50c-fa880fc4bebc",
   "metadata": {},
   "source": [
    "For more information about `Hist`, see https://hist.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79f50a-94ef-463a-9ea0-f3aa2a390051",
   "metadata": {},
   "source": [
    "## Fitting the MC signal\n",
    "The `get_histogram` function returns the histogram values, associated variances, and bin centres. The histogram values of the Hyy MC can be used to fit to a model — e.g., a Gaussian for the Higgs mass peak.\n",
    "\n",
    "For fitting using the `lmfit` library, you may refer to the documentation: https://lmfit.github.io/lmfit-py/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108618e9-bc00-40a1-9a91-9e95301e391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set histogram parameters (Signal)\n",
    "num_bins = 20 \n",
    "xmin, xmax = 120, 130\n",
    "\n",
    "# Extract mass data from GamGam events\n",
    "signal_mass = data['Hyy_0_5']['mass']\n",
    "signal_weight = data['Hyy_0_5']['totalWeight']\n",
    "signal_hist_name = 'signal mass'\n",
    "\n",
    "# Get histogram values and bin centres - in this case, the variance is not needed\n",
    "signal_x, signal_x_err, bin_centres = get_histogram(signal_mass, num_bins, xmin, xmax, signal_hist_name, weight=signal_weight)\n",
    "#signal_x_err = np.sqrt(signal_x) # Statistical uncertainties (Poisson)\n",
    "print(signal_x)\n",
    "print(signal_x_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6462b-5ae0-4269-95c4-d69a0f65f560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f81968-9373-4857-b8f9-1e509a008df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model components (Signal)\n",
    "\n",
    "gaussian_model = GaussianModel() # Gaussian\n",
    "\n",
    "\n",
    "# Set initial guesses for Gaussian peak\n",
    "parameters = gaussian_model.guess(\n",
    "    signal_x, # data to use to guess parameter values\n",
    "    x=bin_centres, amplitude=100, center=125, sigma=2\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "out = gaussian_model.fit(signal_x, # data to be fit\n",
    "                parameters, # guesses for the parameters\n",
    "                x=bin_centres, weights=1/signal_x_err) #ASK\n",
    "\n",
    "# Print out a summary of the fit results\n",
    "print(fit_report(out))\n",
    "\n",
    "# Extract the full best-fit curve (background + signal)\n",
    "fit = out.best_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2abbc-26c5-47cb-bb7e-bcd22dd5b9ce",
   "metadata": {},
   "source": [
    "The resulting best-fit curve can be overlaid on the plot by passing it to the `plot_stacked_hist` function via the `fit` argument.  \n",
    "You can customise how the fit is displayed using the optional parameters:\n",
    "- `fit_fmt` : linestyle and color for the fit (default is `'-r'` for a red solid line)\n",
    "- `fit_label` : legend label for the fit (default is `'fit'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871e417-d391-4059-bf12-30d6128ab886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replot the mass distribution and superimpose the fit\n",
    "# N.B. Use the same num_bins, xmin, xmax as the ones used to make the fit!\n",
    "plot_dict = {\n",
    "    'Signal' : data['Hyy_0_5']\n",
    "}\n",
    "color_list = ['b'] # blue (signal)\n",
    "fig, hist = plot_stacked_hist( plot_dict, plot_variable, color_list, num_bins, xmin, xmax, x_label, fit=fit, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bacdffc-3584-4db5-b640-6bd9861964dd",
   "metadata": {},
   "source": [
    "If you take a closer look, you might notice that a single gaussian does not provide a very good description of the peak in the signal mass distribution!  \n",
    "This would become even more apparent if you try to widen the range of masses fitted.\n",
    "Perhaps you may want to try exploring more complicated fit functions to get a better description?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de4802-330d-40cc-bfa0-21ce5abc6e25",
   "metadata": {},
   "source": [
    "## Fitting the data\n",
    "The `get_histogram` function returns the histogram values, associated variances, and bin centres. The histogram values of the data can be used to fit to a model.\n",
    "You'll start off by fitting a smooth, monotonically decreasing function such as a polynomial model for the 'background'. \n",
    "In order to avoid 'biassing' your results you'll want to 'blind' the region in mass where we expect to see evidence of a Higgs signal and try out different background models.\n",
    "Once you've come up with a set of acceptable background fits you will 'unblind' the signal region and add a Gaussian for the Higgs mass peak.\n",
    "\n",
    "For fitting using the `lmfit` library, you may refer to the documentation: https://lmfit.github.io/lmfit-py/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6638b1f-a6ec-4f0c-9414-4212f8d33298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set histogram parameters\n",
    "num_bins = 200 \n",
    "xmin, xmax = 100, 200\n",
    "\n",
    "# Extract mass data from GamGam events\n",
    "data_mass = data['GamGam_0_5']['mass']\n",
    "data_hist_name = 'data mass'\n",
    "\n",
    "# Get histogram values and bin centres - in this case, the variance is not accessible directly and has to be calculated separately\n",
    "data_x, _, bin_centres = get_histogram(data_mass, num_bins, xmin, xmax, data_hist_name)\n",
    "data_x_err = np.sqrt(data_x) # Statistical uncertainties (Poisson)\n",
    "print(data_x)\n",
    "print(data_x_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d607df3-7b0f-4e83-a5d6-299bfe613901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model components\n",
    "polynomial_model = PolynomialModel(4) # 4th order polynomial\n",
    "#gaussian_model = GaussianModel() # Gaussian\n",
    "\n",
    "# Set initial guesses for polynomial parameters:\n",
    "# c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "parameters = polynomial_model.guess(\n",
    "    data_x, # data to use to guess parameter values\n",
    "    x=bin_centres, c0=max(data_x), c1=0, c2=0, c3=0, c4=0\n",
    ")\n",
    "\n",
    "# Set initial guesses for Gaussian peak\n",
    "# parameters += gaussian_model.guess(\n",
    "#     data_x, # data to use to guess parameter values\n",
    "#     x=bin_centres, amplitude=100, center=125, sigma=2\n",
    "# )\n",
    "\n",
    "# Combine the models\n",
    "#model = polynomial_model + gaussian_model\n",
    "model = polynomial_model \n",
    "\n",
    "# Fit the model to the data\n",
    "out = model.fit(data_x, # data to be fit\n",
    "                parameters, # guesses for the parameters\n",
    "                x=bin_centres, weights=1/data_x_err, nan_policy='omit') #ASK\n",
    "\n",
    "# Print out a summary of the fit results\n",
    "print(fit_report(out))\n",
    "\n",
    "# Extract background parameters from the fit result\n",
    "params_dict = out.params.valuesdict()\n",
    "c0 = params_dict['c0']\n",
    "c1 = params_dict['c1']\n",
    "c2 = params_dict['c2']\n",
    "c3 = params_dict['c3']\n",
    "c4 = params_dict['c4']\n",
    "\n",
    "# Get the background-only part of the fit to data\n",
    "background = (c0 + c1*bin_centres + c2*bin_centres**2 +\n",
    "              c3*bin_centres**3 + c4*bin_centres**4)\n",
    "\n",
    "# Extract the full best-fit curve (background + signal)\n",
    "fit = out.best_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec089194-505f-41d5-a8b8-c61328040f74",
   "metadata": {},
   "source": [
    "The resulting best-fit curve can be overlaid on the plot by passing it to the `plot_stacked_hist` function via the `fit` argument.  \n",
    "You can customise how the fit is displayed using the optional parameters:\n",
    "- `fit_fmt` : linestyle and color for the fit (default is `'-r'` for a red solid line)\n",
    "- `fit_label` : legend label for the fit (default is `'fit'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ad58d-3ec7-4363-9e7c-8c0ec1e89772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replot the mass distribution and superimpose the fit\n",
    "# N.B. Use the same num_bins, xmin, xmax as the ones used to make the fit!\n",
    "plot_dict = {\n",
    "    'Data' : data['GamGam_0_5'],\n",
    "}\n",
    "\n",
    "# Variable to plot on the x-axis\n",
    "plot_variable = 'mass'\n",
    "\n",
    "# Define plot appearance\n",
    "color_list = ['k'] # Black (data)\n",
    "fig, hist = plot_stacked_hist(\n",
    "    plot_dict, plot_variable, color_list,\n",
    "    num_bins, xmin, xmax, x_label,\n",
    "    fit=fit, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590971ba-dd35-4ec1-bfc1-68e5a847d1cb",
   "metadata": {},
   "source": [
    "If you take a closer look, you might notice a *dip* in the mass distribution rather than a peak! This suggests that the current selection cuts may not be sufficient to suppress background, or that more data is needed.\n",
    "\n",
    "Perhaps you may want to plot additional variables to identify where the selection cut could be tightened. You can use the `plot_histograms` function, as shown in the example below.  \n",
    "Note that this function returns:\n",
    "- A **list** of `Figure` objects (one per variable plotted)\n",
    "- A corresponding **list of lists** of `Hist` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610e3b7-ee8e-446b-a2e3-c9042a1efc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot many variables at once \n",
    "\n",
    "# Define what to samples plot: 'GamGam' as data, 'Hyy' as signal\n",
    "plot_dict = {\n",
    "    'Data' : data['GamGam_0_5'],\n",
    "    'Signal' : data['Hyy_0_5']\n",
    "}\n",
    "\n",
    "# Define what variables to plot\n",
    "plot_variables = ['photon_pt[0]', 'photon_pt[1]', 'photon_eta[0]', 'photon_eta[1]']\n",
    "xmin_xmax_list = [(0, 160), (0, 160), (-3, 3), (-3, 3)] # Bin range for each variable\n",
    "color_list = ['k', 'b'] # Black (data) and blue (MC)\n",
    "num_bins_list = 300 # If you give an int instead of a list, this number of bins will be applied to all variables\n",
    "x_label_list = ['photon_pt[0] [GeV]', 'photon_pt[1] [GeV]', 'photon_eta[0]', 'photon_eta[1]']\n",
    "\n",
    "plot_histograms(\n",
    "    plot_dict,\n",
    "    plot_variables,\n",
    "    color_list,\n",
    "    xmin_xmax_list, # If you provide a tuple of 2 numbers, that will be applied to all plots\n",
    "    num_bins_list,\n",
    "    x_label_list,\n",
    "    # Optional arguments start from here\n",
    "    # y_label_list=None, # Str or list of str for y-axis label\n",
    "    # ylim_list=None, # Tuple of 2 numbers or list of tuples for y axis limit\n",
    "    logy=True, # Whether to set the y axis as log scale\n",
    "    # title_list=None, # Str or list of str for title\n",
    "    # marker='o', # Marker type\n",
    "    # title_fontsize=17, # Fontsize for title\n",
    "    # label_fontsize=17, # Fontsize for x and y axes\n",
    "    # legend_fontsize=17, # Fontsize for legend\n",
    "    # tick_labelsize=15, # Fontsize for x and y axes ticks\n",
    "    # text_fontsize=14, # Fontsize for text that shows histogram info\n",
    "    # fig_size=(12, 8), # Figure size\n",
    "    # show_text=False, # Whether to show the text that displays histogram info\n",
    "    # show_back_unc=True, # Whether to show the background uncertainty\n",
    "    # residual_plot=False # Whether to show residual plot under the main plot\n",
    "    # residual_ylim_list=None # Tuple of 2 numbers or list of tuples for residual plot y-axis limit\n",
    ")         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fcfcc-8089-4526-99e6-2ac188036808",
   "metadata": {},
   "source": [
    "There are some arguments worth pointing out:\n",
    "- `xmin_xmax_list` : a tuple of two numbers applied to all plots, or a list of tuples (one per variable)\n",
    "- `num_bins_list` : an int applied to all plots, or a list of int (one per variable)\n",
    "\n",
    "The same applies to these optional arguments: `y_label_list`, `ylim_list`, `title_list`, and `residual_ylim_list`. These can be specified as a single value applied to all plots or as a list matching the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283374c-495d-43e3-9c1e-19df43b40406",
   "metadata": {},
   "source": [
    "## 2D Histogram\n",
    "If you want to plot a 2D histogram, you can use the `histogram_2d` function as shown in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce088ce-2040-4896-9f50-d77091f22b1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eta and phi of the first photon (index 0) in each event\n",
    "data_eta0 = data['GamGam_0_5']['photon_eta'][:, 0]\n",
    "data_phi0 = data['GamGam_0_5']['photon_phi'][:, 0]\n",
    "\n",
    "data_2d = (data_eta0, data_phi0) # Plot eta on x-axis, phi on y-axis\n",
    "num_bins_2d = (100, 100) # Number of bins along x and y-axis \n",
    "min_max_2d = ((-3, 3), (-4, 4)) # The bin range in x and y\n",
    "label_2d = ('$\\\\eta$ [0]', '$\\\\phi$ [0]') # The labels for x and y-axis\n",
    "\n",
    "# Plot the 2D histogram\n",
    "fig, h = histogram_2d(data_2d, num_bins_2d,\n",
    "                      min_max_2d, label_2d,\n",
    "                      # Optional arguments\n",
    "                      # label_fontsize=12, tick_labelsize=10,\n",
    "                      # title_fontsize=13, title='',\n",
    "                      # colorbar_label='Events'\n",
    "                     )\n",
    "# Uncomment to save the image\n",
    "#fig.savefig('test.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f91bb-fb8f-4281-a146-4f9b9673f2cb",
   "metadata": {},
   "source": [
    "## Write the Data to Disk\n",
    "When working with larger datasets, e.g. using a higher `fraction` in `analysis_parquet`, it is often more efficient to write the data to disk rather than keeping it all in memory. \n",
    "\n",
    "To do this, set the optional argument `write_parquet=True`. This will instruct the `analysis_parquet` function to save the output as Parquet files. If you write data to disk, it is recommended to set `return_output=False` to avoid loading large amounts of data into memory.  \n",
    "\n",
    "In addition, you can customise the output location using the `output_directory` argument. If not provided, the files will be saved in an `output` folder with a unique name created from the current date and time.  \n",
    "\n",
    "**Note**: The provided output directory must be unique and cannot be reused. Attempting to write to an existing directory will raise a `FileExistsError` to prevent accidental overwriting of data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553de47e-c1ec-4d0a-a297-eb8f5fca3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own di-photon preselection to parquet files\n",
    "string_code_list = ['GamGam', 'Hyy'] # List of dataset codes to load\n",
    "\n",
    "# Define by hand the variables to read from the dataset\n",
    "# read_variables = ['photon_pt', 'photon_eta', 'photon_phi', 'photon_e', 'photon_n']\n",
    "\n",
    "# Alternatively, you may wish to use the pre-defined function `validate_read_variables` to find the names of all available variables\n",
    "read_variables = get_valid_variables('GamGam') # Save all available variables\n",
    "\n",
    "# Specify the directory to which the output parquet files should be written\n",
    "output_dir = '../../output-parquet/diphoton_PtGT15'\n",
    "\n",
    "# Custom selection cut function to filter the data when writing out parquet files\n",
    "def preselection_cut(data):\n",
    "    # Cut on photon number\n",
    "    cut_photon_n = (data['photon_n'] == 2) # Define mask\n",
    "    data = data[cut_photon_n] # Only keep events that have two photons\n",
    "\n",
    "    # Cut on photon pt\n",
    "    photon_pt0 = data['photon_pt'][:, 0] \n",
    "    photon_pt1 = data['photon_pt'][:, 1]\n",
    "    # Use bitwise operator '&' for AND, '|' for OR. Remember the parentheses!\n",
    "    cut_photon_pt = (photon_pt0 > 20) & (photon_pt1 > 20)\n",
    "    data = data[cut_photon_pt] # Keep events where two photons have pt > 20 GeV\n",
    "\n",
    "    # Define four momentum\n",
    "    four_momentum = vector.zip({\n",
    "        'pt': data['photon_pt'],\n",
    "        'eta' : data['photon_eta'],\n",
    "        'phi' : data['photon_phi'],\n",
    "        'E' : data['photon_e']\n",
    "    })\n",
    "    # Add the 4-momentum of the two photons in each event and get the  invariant mass using .M\n",
    "    data['mass'] = (four_momentum[:, 0] + four_momentum[:, 1]).M\n",
    "    \n",
    "    # Cut on di-photon mass\n",
    "    cut_low = 15\n",
    "    cut_high = 1000   \n",
    "    cut_mass = (data['mass']>cut_low) &  (data['mass']<cut_high)\n",
    "    data = data[cut_mass] # Select events satisfying the condition: cut_low < mass < cut_high (GeV)\n",
    "    \n",
    "    return data\n",
    "\n",
    "analysis_parquet(read_variables, string_code_list, fraction=1,\n",
    "                cut_function=preselection_cut, write_parquet=True,\n",
    "                output_directory=output_dir, return_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6726654-b13d-4bc5-aeac-2d49592b4997",
   "metadata": {},
   "source": [
    "Files saved by `analysis_parquet` can be reloaded using the same function by specifying the following optional arguments:\n",
    "- `read_directory` : Path to the directory where the data is saved.\n",
    "- `subdirectory_names` : A list of subdirectories (folder names) to read from. If not provided, all subdirectories in `read_directory` will be read.\n",
    "  \n",
    "Note that `string_code_list` is also an optional argument with a default of `None`. Either `string_code_list` _**or**_ `read_directory` must be provided to the `analysis_parquet` function.  \n",
    "\n",
    "**_Tip_**: Before loading large datasets, it is recommended to restart the kernel to free up memory. Avoid reading too many variables at once to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f137d3f-b01b-42e6-a240-2b678a93f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_directory = '../../output-parquet/diphoton_PtGT15' # Directory where data is saved\n",
    "read_variables = ['mass'] # Variables to read\n",
    "\n",
    "fraction = 1\n",
    "\n",
    "# Custom selection cut function to filter the data\n",
    "def read_parquet_cut(data):\n",
    "\n",
    "    # Cut on di-photon mass\n",
    "    cut_low = 50\n",
    "    cut_high = 200   \n",
    "    cut_mass = (data['mass']>cut_low) &  (data['mass']<cut_high)\n",
    "    data = data[cut_mass] # Select events satisfying the condition: cut_low < mass < cut_high (GeV) \n",
    "    \n",
    "    return data\n",
    " \n",
    "\n",
    "data = analysis_parquet(read_variables, string_code_list=None, read_directory=read_directory,\n",
    "                        subdirectory_names=None, fraction=fraction, cut_function=read_parquet_cut,\n",
    "                        write_parquet=False, output_directory=None, return_output=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da313b1-0206-4925-a5a9-e88a1f7bc8e2",
   "metadata": {},
   "source": [
    "Note the 'change' in the keys of the output dictionary. Each key is constructed by combining the subdirectory name with the `fraction`, separated with`' x'`.  \n",
    "For example, if the subdirectory name is `'Hyy_0_1'` and `fraction = 0.1`, the resulting key will be `'Hyy_0_1 x0_1'`, implying that the sample represents `0.1 × 0.1` of the original dataset.  \n",
    "However, this naming convention does not reflect whether any cuts have been applied, hence the actual number of events is likely not this fraction of the full data —  it only provides a rough estimate. So, make sure to **record** your output directories and cuts clearly in your lab book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea6e81-d1be-48fb-bdb6-7cd658804632",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Function 'produced_event_count' returns the total number (sum of weights) of generated Monte Carlo events for the data set specified by 'dataKey' and the specificed integrated luminosity.\n",
    "# You will need this as the denominator in the calculation of the selection efficiency for your signal processes\n",
    "luminosity = 36.6\n",
    "produced_event_count('Hyy',luminosity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
